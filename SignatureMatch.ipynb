{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignatureMatch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+YqvcU2q8cAKo8hbVkKc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sobil-dalal/SignatureMatch/blob/main/SignatureMatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhvkfHfzw3r_"
      },
      "source": [
        "## Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xFcS8zYw2ZI",
        "outputId": "fa2e94f1-4ebf-4d95-ef60-bf1da060bb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/sobil-dalal/SignatureMatch.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SignatureMatch'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aalAP7BE3zhU"
      },
      "source": [
        "## Changing working directory location to SignatureMatch directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46zIacj839ZX"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/SignatureMatch\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx368U7kuUJF"
      },
      "source": [
        "## Load Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YtPYWFtFLb"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjiJvMDJzXXx",
        "outputId": "1bd4867b-a055-4cdd-a151-c460b05d37e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras as kr\n",
        "print(tf.__version__)\n",
        "print(kr.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXM2bJC7RUzY"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCnN04XVRW5Q",
        "outputId": "152ce3a8-1879-4c43-f41a-e63a1dbad29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://www.iapr-tc11.org/dataset/ICDAR_SignatureVerification/SigComp2011/sigComp2011-trainingSet.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 00:52:00--  http://www.iapr-tc11.org/dataset/ICDAR_SignatureVerification/SigComp2011/sigComp2011-trainingSet.zip\n",
            "Resolving www.iapr-tc11.org (www.iapr-tc11.org)... 157.16.221.56\n",
            "Connecting to www.iapr-tc11.org (www.iapr-tc11.org)|157.16.221.56|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245154271 (234M) [application/zip]\n",
            "Saving to: ‘sigComp2011-trainingSet.zip’\n",
            "\n",
            "sigComp2011-trainin 100%[===================>] 233.80M  17.6MB/s    in 15s     \n",
            "\n",
            "2020-11-06 00:52:15 (16.1 MB/s) - ‘sigComp2011-trainingSet.zip’ saved [245154271/245154271]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0VEAKNuCsCZ",
        "outputId": "b407e6a9-9153-4bfb-beea-16826ac4875c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/SignatureMatch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdYfuiGZB4vM"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('sigComp2011-trainingSet.zip', 'r')\n",
        "zip_ref.extractall(os.getcwd(),pwd='I hereby accept the SigComp 2011 disclaimer.'.encode())\n",
        "zip_ref.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3vtn1uG__w-",
        "outputId": "31e488f9-3594-4112-a445-9d6791fbcc95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://www.iapr-tc11.org/dataset/ICDAR_SignatureVerification/SigComp2011/sigComp2011-test.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 00:41:26--  http://www.iapr-tc11.org/dataset/ICDAR_SignatureVerification/SigComp2011/sigComp2011-test.zip\n",
            "Resolving www.iapr-tc11.org (www.iapr-tc11.org)... 157.16.221.56\n",
            "Connecting to www.iapr-tc11.org (www.iapr-tc11.org)|157.16.221.56|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 566029024 (540M) [application/zip]\n",
            "Saving to: ‘sigComp2011-test.zip’\n",
            "\n",
            "sigComp2011-test.zi 100%[===================>] 539.81M  16.8MB/s    in 34s     \n",
            "\n",
            "2020-11-06 00:42:00 (16.1 MB/s) - ‘sigComp2011-test.zip’ saved [566029024/566029024]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GGFfYZ41FJ1"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFhKF2yz0OLW"
      },
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoFalG-OOuZA",
        "outputId": "b073e5ba-d0a4-4e58-86ba-79ab1d50ae05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Total Params:\", FRmodel.count_params())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 3743280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7ccRa4VQYo3"
      },
      "source": [
        "## Implement triplet loss function\n",
        "\n",
        "\n",
        "\n",
        "For an image $x$, we denote its encoding $f(x)$, where $f$ is the function computed by the neural network.\n",
        "\n",
        "\n",
        "\n",
        "<!--\n",
        "We will also add a normalization step at the end of our model so that $\\mid \\mid f(x) \\mid \\mid_2 = 1$ (means the vector of encoding should be of norm 1).\n",
        "!-->\n",
        "\n",
        "Training will use triplets of images $(A, P, N)$:  \n",
        "\n",
        "- A is an \"Anchor\" image--a picture of a person. \n",
        "- P is a \"Positive\" image--a picture of the same person as the Anchor image.\n",
        "- N is a \"Negative\" image--a picture of a different person than the Anchor image.\n",
        "\n",
        "These triplets are picked from our training dataset. We will write $(A^{(i)}, P^{(i)}, N^{(i)})$ to denote the $i$-th training example. \n",
        "\n",
        "You'd like to make sure that an image $A^{(i)}$ of an individual is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\\alpha$:\n",
        "\n",
        "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
        "\n",
        "You would thus like to minimize the following \"triplet cost\":\n",
        "\n",
        "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
        "\n",
        "Here, we are using the notation \"$[z]_+$\" to denote $max(z,0)$.  \n",
        "\n",
        "Notes:\n",
        "- The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small. \n",
        "- The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.\n",
        "- $\\alpha$ is called the margin. It is a hyperparameter that you pick manually. We will use $\\alpha = 0.2$. \n",
        "\n",
        "Most implementations also rescale the encoding vectors to haven L2 norm equal to one (i.e., $\\mid \\mid f(img)\\mid \\mid_2$=1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9AHT8MDQeNU"
      },
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    \"\"\"\n",
        "    Implementation of the triplet loss as defined by formula\n",
        "    \n",
        "    Arguments:\n",
        "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
        "    y_pred -- python list containing three objects:\n",
        "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
        "            positive -- the encodings for the positive images, of shape (None, 128)\n",
        "            negative -- the encodings for the negative images, of shape (None, 128)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- real number, value of the loss\n",
        "    \"\"\"\n",
        "    \n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "      \n",
        "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
        "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
        "    # Step 3: subtract the two previous distances and add alpha.\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
        "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
        "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHYIiEXKQIng"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl1HB8OiQOa7"
      },
      "source": [
        "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}